## Assignment 1:

from pyspark import SparkContext
sc = SparkContext("local", "AverageWeight")
avg_rdd = sc.textFile('/FileStore/tables/cars.tsv').map(lambda x: x.split('\t')).filter(lambda x: x[9] == 'American').map(lambda x: (x[0], (int(x[6]), 1))).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).mapValues(lambda x: x[0] // x[1]) 
output = avg_rdd.collect() 
for make, avg_weight in output: 
	print(f"({make}, {avg_weight})")

## Assignment 2:

from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("TopProductsByVolume").getOrCreate()

# Read the CSV file
file_path = "/FileStore/tables/OnlineRetail-1.csv"
data = spark.read.csv(file_path, header=True, inferSchema=True)

# Create a temporary SQL table
data.createOrReplaceTempView("online_retail")

# Run Spark SQL query to get the top products by volume
query = """
    SELECT StockCode, SUBSTRING(InvoiceDate, 7, 4) || '-' || SUBSTRING(InvoiceDate, 1, 2) AS InvoiceMonth,
           SUM(Quantity) AS TotalQuantity
    FROM online_retail
    GROUP BY StockCode, InvoiceMonth
    ORDER BY TotalQuantity DESC
    LIMIT 25
"""

result = spark.sql(query)

# Show the results
result.show()





## Assignment 3

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Create a Spark session
spark = SparkSession.builder.appName("TotalSaleAnalysis").getOrCreate()

# Read the CSV file
file_path = "/FileStore/tables/OnlineRetail-1.csv"
data = spark.read.csv(file_path, header="True", inferSchema="True")

# Preprocess data to calculate sales value and extract year and month
data = data.withColumn("SalesValue", round(col("Quantity") * col("UnitPrice"), 2))\
           .withColumn("InvoiceMonth", to_date(col("InvoiceDate"), "M/d/yyyy H:mm").cast("date"))\
           .filter(year("InvoiceMonth") == 2011)

# Calculate total sale value and total orders per month
monthly_sales = data.groupBy("InvoiceMonth")\
                    .agg(round(sum("SalesValue"), 1).alias("TotalSaleValue"),
                         countDistinct("InvoiceNo").alias("TotalOrders"))\
                    .sort(col("TotalSaleValue").desc())

# Show the results
monthly_sales.show()

## Assignment 4

from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp
from pyspark.sql.streaming import StreamingQuery

# Create a Spark session
spark = SparkSession.builder.appName("WordStream").getOrCreate()

# Create a DataFrame representing the stream of input lines from socket
lines = spark.readStream.format("socket").option("host", "localhost").option("port", 9999).load()

# Split the lines into words
words = lines.selectExpr("explode(split(value, ' ')) as word")

# Add timestamp to each word
words_with_timestamp = words.withColumn("timestamp", current_timestamp())

# Display the word and its corresponding timestamp on the console
query: StreamingQuery = words_with_timestamp.writeStream \
    .outputMode("append") \
    .format("console") \
    .trigger(processingTime="5 seconds") \
    .start()

query.awaitTermination()

## Assignment 5

from pyspark.sql import SparkSession
from pyspark.sql.functions import to_json, struct
from pyspark.sql.types import StructType, StructField, DoubleType, TimestampType

# Create a Spark session
spark = SparkSession.builder.appName("KafkaWriter").getOrCreate()

# Define schema for the data
schema = StructType([
    StructField("timestamp", TimestampType()),
    StructField("value", DoubleType())
])

# Create a streaming DataFrame from the Rate source
rateDF = spark.readStream.format("rate").option("rowsPerSecond", 5).load()

# Transform the data and write to Kafka
query = rateDF.selectExpr("timestamp AS key", "to_json(struct(*)) AS value") \
    .writeStream.format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("topic", "topic1") \
    .option("checkpointLocation", "/tmp/checkpoint") \
    .start()

query.awaitTermination()
